{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Optimal M: 1.3422286033108315\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load in Data\n",
    "x_file = 'synthetic_data/batch_1/x_gen_syn_n3_p10_corr0.5_snr5_seed2022_0.csv'\n",
    "y_file = 'synthetic_data/batch_1/y_gen_syn_n3_p10_corr0.5_snr5_seed2022_0.csv'\n",
    "x = np.loadtxt(x_file, delimiter=\",\")\n",
    "y = np.loadtxt(y_file, delimiter=\",\")\n",
    "l0 = 0.01\n",
    "l2 = 0\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Fit a linear regression model to estimate coefficients (betas)\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(x, y)\n",
    "\n",
    "# Calculate the maximum absolute coefficient value\n",
    "max_abs_beta = np.max(np.abs(linear_model.coef_))\n",
    "\n",
    "# Calculate the optimal M value as 1.5 times the maximum absolute coefficient value\n",
    "optimal_M = 1.5 * max_abs_beta\n",
    "\n",
    "print(f\"Optimal M: {optimal_M}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retro Branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Rough Visualization of Tree Structure\n",
    "\n",
    "def hierarchy_pos(G, root=None, width=1., vert_gap = 0.1, vert_loc = 0, xcenter = 0.5):\n",
    "    pos = _hierarchy_pos(G, root, width, vert_gap, vert_loc, xcenter)\n",
    "    return pos\n",
    "\n",
    "def _hierarchy_pos(G, root, width=2., vert_gap = 0.1, vert_loc = 0, xcenter = 0.5, pos = None, parent = None, parsed = []):\n",
    "    if pos is None:\n",
    "        pos = {root: (xcenter, vert_loc)}\n",
    "    else:\n",
    "        pos[root] = (xcenter, vert_loc)\n",
    "    \n",
    "    children = list(G.neighbors(root))\n",
    "    if not isinstance(G, nx.DiGraph) and parent is not None:\n",
    "        children.remove(parent)  \n",
    "            \n",
    "    if len(children) != 0:\n",
    "        dx = width / len(children) \n",
    "        nextx = xcenter - width/2 - dx/2\n",
    "        for child in children:\n",
    "            nextx += dx\n",
    "            pos = _hierarchy_pos(G, child, width = dx, vert_gap = vert_gap, vert_loc = vert_loc-vert_gap, xcenter=nextx, pos=pos, parent=root, parsed=parsed)\n",
    "    \n",
    "    return pos\n",
    "\n",
    "def visualize_tree(root):\n",
    "    if not root:\n",
    "        return\n",
    "\n",
    "    tree_graph = nx.DiGraph()\n",
    "\n",
    "    def add_nodes_edges(node):\n",
    "        if node.left:\n",
    "            tree_graph.add_edge(node.node_key, node.left.node_key)\n",
    "            add_nodes_edges(node.left)\n",
    "        if node.right:\n",
    "            tree_graph.add_edge(node.node_key, node.right.node_key)\n",
    "            add_nodes_edges(node.right)\n",
    "\n",
    "    add_nodes_edges(root)\n",
    "\n",
    "    pos = hierarchy_pos(tree_graph, root.node_key)\n",
    "    nx.draw(tree_graph, pos=pos, with_labels=True, node_size=100, node_color=\"skyblue\", font_size=6, font_weight='bold')\n",
    "    plt.title(\"Binary Tree Visualization\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/shreyas/Desktop/thesis code/updated/var_selection/tuningNewTree.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shreyas/Desktop/thesis%20code/updated/var_selection/tuningNewTree.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m newProblem \u001b[39m=\u001b[39m newTree\u001b[39m.\u001b[39mProblem(x,y,l0,l2, m \u001b[39m=\u001b[39m \u001b[39m1.5\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shreyas/Desktop/thesis%20code/updated/var_selection/tuningNewTree.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m newTree2 \u001b[39m=\u001b[39m newTree\u001b[39m.\u001b[39mtree(newProblem)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/shreyas/Desktop/thesis%20code/updated/var_selection/tuningNewTree.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m newTree2\u001b[39m.\u001b[39;49mbranch_and_bound()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shreyas/Desktop/thesis%20code/updated/var_selection/tuningNewTree.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# i, reward = tree.branch_and_bound(x,y,l0,l2)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shreyas/Desktop/thesis%20code/updated/var_selection/tuningNewTree.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shreyas/Desktop/thesis%20code/updated/var_selection/tuningNewTree.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# def count_nodes(node):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shreyas/Desktop/thesis%20code/updated/var_selection/tuningNewTree.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# pairs = tree.get_state_pairs(test_tree.root)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shreyas/Desktop/thesis%20code/updated/var_selection/tuningNewTree.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# print(f'Number of Edges/State Pairs: {len(pairs)}')\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/thesis code/updated/var_selection/newTree.py:395\u001b[0m, in \u001b[0;36mtree.branch_and_bound\u001b[0;34m(self, branch)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbranch_and_bound\u001b[39m(\u001b[39mself\u001b[39m, branch\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 395\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstart_root(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    397\u001b[0m     fin_solving \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    398\u001b[0m     iters \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/thesis code/updated/var_selection/newTree.py:207\u001b[0m, in \u001b[0;36mtree.start_root\u001b[0;34m(self, warm_start)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactive_nodes[\u001b[39m'\u001b[39m\u001b[39mroot_node\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m root_node\n\u001b[1;32m    206\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_nodes[\u001b[39m'\u001b[39m\u001b[39mroot_node\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m root_node\n\u001b[0;32m--> 207\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproblem\u001b[39m.\u001b[39;49mlower_solve(root_node)\n\u001b[1;32m    208\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproblem\u001b[39m.\u001b[39mupper_solve(root_node)\n\u001b[1;32m    210\u001b[0m \u001b[39m# Update bounds and opt gap\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/thesis code/updated/var_selection/newTree.py:62\u001b[0m, in \u001b[0;36mProblem.lower_solve\u001b[0;34m(self, node, solver, rel_tol, int_tol, tree_upper_bound, mio_gap, cd_max_itr, kkt_max_itr)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39mSolves the lower bound problem for a given node in the branch and bound tree.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39m    float: The dual value of the solution.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39mif\u001b[39;00m solver \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml1cd\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m     sol \u001b[39m=\u001b[39m cd_solve(x\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx, y\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my, l0\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml0, l2\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml2, m\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mm, zlb\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mzlb,\n\u001b[1;32m     63\u001b[0m                    zub\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mzub, xi_norm\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mxi_norm, rel_tol\u001b[39m=\u001b[39;49mrel_tol,\n\u001b[1;32m     64\u001b[0m                    warm_start\u001b[39m=\u001b[39;49mnode\u001b[39m.\u001b[39;49mwarm_start, r\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mr,\n\u001b[1;32m     65\u001b[0m                    tree_upper_bound\u001b[39m=\u001b[39;49mtree_upper_bound, mio_gap\u001b[39m=\u001b[39;49mmio_gap,\n\u001b[1;32m     66\u001b[0m                    gs_xtr\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgs_xtr, gs_xb\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgs_xb,\n\u001b[1;32m     67\u001b[0m                    cd_max_itr\u001b[39m=\u001b[39;49mcd_max_itr, kkt_max_itr\u001b[39m=\u001b[39;49mkkt_max_itr)\n\u001b[1;32m     68\u001b[0m     node\u001b[39m.\u001b[39mprimal_value \u001b[39m=\u001b[39m sol\u001b[39m.\u001b[39mprimal_value\n\u001b[1;32m     69\u001b[0m     node\u001b[39m.\u001b[39mdual_value \u001b[39m=\u001b[39m sol\u001b[39m.\u001b[39mdual_value\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/varSelection/lib/python3.9/site-packages/l0bnb/relaxation/core.py:161\u001b[0m, in \u001b[0;36msolve\u001b[0;34m(x, y, l0, l2, m, zlb, zub, gs_xtr, gs_xb, xi_norm, warm_start, r, rel_tol, tree_upper_bound, mio_gap, check_if_integral, cd_max_itr, kkt_max_itr)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     cur_gap \u001b[39m=\u001b[39m (tree_upper_bound \u001b[39m-\u001b[39m cost) \u001b[39m/\u001b[39m tree_upper_bound\n\u001b[0;32m--> 161\u001b[0m \u001b[39mif\u001b[39;00m cur_gap \u001b[39m<\u001b[39;49m mio_gap \u001b[39mand\u001b[39;00m tree_upper_bound \u001b[39m>\u001b[39m dual_cost:\n\u001b[1;32m    162\u001b[0m     \u001b[39mif\u001b[39;00m ((cost \u001b[39m-\u001b[39m dual_cost) \u001b[39m/\u001b[39m \u001b[39mabs\u001b[39m(cost) \u001b[39m<\u001b[39m rel_tol) \u001b[39mor\u001b[39;00m \\\n\u001b[1;32m    163\u001b[0m             (cd_tol \u001b[39m<\u001b[39m \u001b[39m1e-8\u001b[39m \u001b[39mand\u001b[39;00m check_if_integral):\n\u001b[1;32m    164\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "import newTree\n",
    "# Solve using Branch and Bound\n",
    "newProblem = newTree.Problem(x,y,l0,l2, m = 1.5)\n",
    "newTree2 = newTree.tree(newProblem)\n",
    "newTree2.branch_and_bound()\n",
    "# i, reward = tree.branch_and_bound(x,y,l0,l2)\n",
    "\n",
    "# def count_nodes(node):\n",
    "#     \"\"\"Count the number of nodes in the tree.\"\"\"\n",
    "#     if not node:\n",
    "#         return 0\n",
    "#     return 1 + count_nodes(node.left) + count_nodes(node.right)\n",
    "\n",
    "# print(f'Number of iterations: {i}')\n",
    "# print(f'Number of Nodes in Tree: {count_nodes(test_tree.root)}')\n",
    "# pairs = tree.get_state_pairs(test_tree.root)\n",
    "# print(f'Number of Edges/State Pairs: {len(pairs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque, namedtuple\n",
    "import tree \n",
    "from tree import get_state_pairs\n",
    "from settings import MAX_ITERS, EPSILON_START, \\\n",
    "     EPSILON_END, EPSILON_DECAY, BATCH_SIZE, INT_EPS, GAMMA, TARGET_UPDATE\n",
    "\n",
    "\n",
    "# Memory representation of states\n",
    "Transition = namedtuple('Transition', \n",
    "                        ('prev_state', 'state', 'reward'))\n",
    "\n",
    "# Deep Q Network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output1 = F.relu(self.fc1(x))\n",
    "        output2 = F.relu(self.fc2(output1))\n",
    "        output = F.relu(self.fc3(output2))\n",
    "        return(output)\n",
    "\n",
    "# Memory for our agent\n",
    "class Memory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# Agent that performs, remembers and learns actions\n",
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.policy_net = DQN(32)\n",
    "        self.target_net = DQN(32)\n",
    "        self.optimizer = optim.RMSprop(self.policy_net.parameters())\n",
    "        self.memory = Memory(10000)\n",
    "        self.episodes_played = 0\n",
    "        self.epsilon = EPSILON_START\n",
    "        self.epsilon_decay = EPSILON_DECAY\n",
    "        self.epsilon_end = EPSILON_END\n",
    "\n",
    "    def retrobranch(self, tree):\n",
    "        # Complete Tree -- Get (Non-Optimal) States for Leaf Nodes\n",
    "        for node in tree.all_nodes.values():\n",
    "            if node.state is None: \n",
    "                if len(node.support) == 0:\n",
    "                    best_j = 0\n",
    "                # Select an action according to an epsilon greedy approach \n",
    "                elif (random.random() < self.epsilon):\n",
    "                    z = node.z\n",
    "                    support = node.support\n",
    "                    diff = [min(1-z[i], z[i]-0) for i in range(len(support))]\n",
    "                    best_j = support[np.argmax(diff)]\n",
    "                \n",
    "                else:\n",
    "                    support = node.support\n",
    "                    best_val = -math.inf\n",
    "                    best_j = 0\n",
    "\n",
    "                    for i in range(len(support)):\n",
    "                        state = torch.tensor(np.array([tree.get_state(node.node_key, support[i])]), \n",
    "                                            dtype=torch.float)\n",
    "                        # Agent estimates using policy network\n",
    "                        val = self.policy_net(state) \n",
    "                        if val > best_val:\n",
    "                            best_val = val\n",
    "                            best_j = support[i]\n",
    "            \n",
    "                node.state = tree.get_state(node.node_key, best_j)\n",
    "\n",
    "        # Set rewards\n",
    "        total_reward = 0\n",
    "\n",
    "        # Call tree function to create all state to state pairs\n",
    "        state_pairs = get_state_pairs(tree.root)\n",
    "        for prev, curr, r in state_pairs:\n",
    "            total_reward += r\n",
    "\n",
    "            # Add state pairs and reward to memory \n",
    "            self.memory.push(torch.tensor(np.array([prev]), dtype=torch.float), \n",
    "                             torch.tensor(np.array([curr]), dtype=torch.float), \n",
    "                             torch.tensor([r], dtype=torch.float))\n",
    "        \n",
    "        # Update target network\n",
    "        if self.episodes_played % TARGET_UPDATE == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        return total_reward\n",
    "\n",
    "    def select_action(self, T):\n",
    "        # Select an action according to an epsilon greedy approach        \n",
    "        if (random.random() < self.epsilon):\n",
    "            # max fraction branching\n",
    "            best_node_key, best_j = T.max_frac_branch()\n",
    "        else:\n",
    "            # calculate estimated value for all nodes\n",
    "            best_val = -math.inf\n",
    "            best_node_key = None\n",
    "            best_j = 0\n",
    "\n",
    "            for node_key in T.active_nodes:\n",
    "                support = T.active_nodes[node_key].support\n",
    "                for i in range(len(support)):\n",
    "                    if (T.active_nodes[node_key].z[i] < INT_EPS) or (T.active_nodes[node_key].z[i] > 1-INT_EPS):\n",
    "                        continue\n",
    "                    state = torch.tensor(np.array([T.get_state(node_key, support[i])]), dtype=torch.float)\n",
    "                    # Agent estimates usings policy network\n",
    "                    val = self.policy_net(state) \n",
    "                    if(val > best_val):\n",
    "                        best_val = val\n",
    "                        best_node_key = node_key\n",
    "                        best_j = support[i]\n",
    "\n",
    "        return(best_node_key, best_j)\n",
    "    \n",
    "    def replay_memory(self):\n",
    "        # Only Replay Memory if enough enteries in Memory\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        # Sample from our memory\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Concatenate our tensors for previous states\n",
    "        prev_state_batch = torch.cat(batch.prev_state)\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # Predict Q-values for the previous states\n",
    "        pred_q_values = self.policy_net(prev_state_batch)\n",
    "        pred_q_values = pred_q_values.squeeze(1) # Match shape of targets\n",
    "\n",
    "        # Compute expected Q-values based on next states and rewards\n",
    "        with torch.no_grad():\n",
    "            max_next_q_values = torch.flatten(self.target_net(state_batch))\n",
    "            targets = reward_batch + GAMMA * max_next_q_values\n",
    "\n",
    "        # Compute loss\n",
    "        loss_f = nn.MSELoss()\n",
    "        loss = loss_f(pred_q_values, targets)\n",
    "\n",
    "        # Optimization\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "        # Update Parameters\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "def RL_solve(agent, x, y, l0, l2):\n",
    "    # Solving an instance using agent to make choices in tree\n",
    "    T = tree.tree(x,y,l0,l2)\n",
    "    fin_solving = T.start_root(None)\n",
    "    iters = 0\n",
    "\n",
    "    while (fin_solving == False) and (iters < MAX_ITERS):\n",
    "        # Select and perform an action\n",
    "        node, j = agent.select_action(T)\n",
    "        fin_solving, old_gap, new_gap = T.step(node, j) \n",
    "\n",
    "        # Optimize the target network using replay memory\n",
    "        agent.replay_memory()\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    # Store tree in memory and get total reward for tree\n",
    "    tot_reward = agent.retrobranch(T)\n",
    "\n",
    "    # Update number of episodes Agent has played\n",
    "    agent.episodes_played += 1\n",
    "        \n",
    "    return(iters, tot_reward, len(T.best_beta))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize agent\n",
    "agent = Agent()\n",
    "\n",
    "# Solve Using RL Agent\n",
    "iters, tot_reward, nnz = RL_solve(agent, x, y, l0, l2)\n",
    "\n",
    "print(f'Iterations This Episode: {iters}')\n",
    "print(f'Total Reward this Episode: {tot_reward}')\n",
    "print(f'Number of Items in Memory: {len(agent.memory.memory)}')\n",
    "print(\"-----------------------------------------\")\n",
    "print(f'Episodes Played: {agent.episodes_played}')\n",
    "print(f'Epsilon: {round(agent.epsilon,4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of Non-Zero Coeffs: {nnz}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize New Agent\n",
    "agent = model.Agent()\n",
    "\n",
    "total_rewards = []\n",
    "iterations_per_episode = []\n",
    "epsilons = []\n",
    "\n",
    "# Play through 100 episodes\n",
    "for episode in range(100):\n",
    "    iters, tot_reward, test_tree = model.RL_solve(agent, x, y, l0, l2)\n",
    "    total_rewards.append(tot_reward)\n",
    "    iterations_per_episode.append(iters)\n",
    "    epsilons.append(agent.epsilon)\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode} - Total Reward: {tot_reward}, Iterations: {iters}\")\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(18, 8))\n",
    "\n",
    "plt.subplot(2, 4, 1)\n",
    "plt.plot(total_rewards)\n",
    "plt.title('Total Rewards per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "\n",
    "plt.subplot(2, 4, 2)\n",
    "plt.plot(iterations_per_episode)\n",
    "plt.title('Iterations per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Iterations')\n",
    "\n",
    "plt.subplot(2, 4, 3)\n",
    "plt.plot(epsilons)\n",
    "plt.title('Epsilon per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Epsilon')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
